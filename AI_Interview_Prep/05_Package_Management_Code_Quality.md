---
title: "Package Management & Code Quality"
layout: default
parent: "Python & Coding"
nav_order: 5
render_with_liquid: false
---

# Section 5: Package Management & Code Quality for AI Engineers

---

## 5.1 POETRY

### Q: Explain Poetry and why it's preferred for AI/ML projects.

**Answer:**
Poetry is a modern Python dependency management and packaging tool that solves the "works on my machine" problem.

**Key features:**
- **Deterministic builds**: `poetry.lock` ensures identical environments everywhere
- **Virtual environment management**: Automatic venv creation and management
- **Dependency resolution**: Proper constraint solving (unlike pip's greedy approach)
- **Build system**: Can build and publish packages

```toml
# pyproject.toml -- the single source of truth
[tool.poetry]
name = "ai-service"
version = "1.0.0"
description = "Production AI service"
python = "^3.11"

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.109.0"
uvicorn = {extras = ["standard"], version = "^0.27.0"}
openai = "^1.12.0"
anthropic = "^0.18.0"
pydantic = "^2.6.0"
numpy = "^1.26.0"
tiktoken = "^0.6.0"

[tool.poetry.group.dev.dependencies]
pytest = "^8.0.0"
pytest-asyncio = "^0.23.0"
pytest-mock = "^3.12.0"
mypy = "^1.8.0"
ruff = "^0.2.0"
pre-commit = "^3.6.0"
ipython = "^8.21.0"

[tool.poetry.group.ml.dependencies]
# Optional group for ML-specific deps
torch = "^2.2.0"
transformers = "^4.38.0"
sentence-transformers = "^2.3.0"

[tool.poetry.scripts]
serve = "ai_service.main:run_server"
train = "ai_service.training:main"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

**Essential commands:**
```bash
# Setup
poetry init                          # Initialize new project
poetry install                       # Install all dependencies
poetry install --with ml             # Include optional groups
poetry install --without dev         # Production install (no dev deps)

# Dependency management
poetry add openai                    # Add dependency
poetry add --group dev pytest        # Add dev dependency
poetry add torch --optional          # Add optional dependency
poetry remove package-name           # Remove dependency
poetry update                        # Update all deps within constraints
poetry update openai                 # Update specific package
poetry lock                          # Regenerate lock file

# Environment
poetry env info                      # Show environment info
poetry env use python3.11            # Use specific Python version
poetry shell                         # Activate virtual environment
poetry run python script.py          # Run in venv without activating
poetry run pytest                    # Run tests in venv

# Export (for Docker/CI)
poetry export -f requirements.txt --output requirements.txt --without-hashes
```

### Q: How do you handle conflicting ML dependencies (e.g., different PyTorch versions)?

**Answer:**
```toml
# Strategy 1: Dependency groups (Poetry)
[tool.poetry.group.gpu.dependencies]
torch = {version = "^2.2.0", markers = "sys_platform != 'darwin' or platform_machine != 'x86_64'"}

[tool.poetry.group.cpu.dependencies]
torch = {version = "^2.2.0", source = "pytorch-cpu"}

[[tool.poetry.source]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
priority = "supplemental"

# Strategy 2: Environment markers
[tool.poetry.dependencies]
torch = [
    {version = "^2.2.0", platform = "linux", source = "pytorch-gpu"},
    {version = "^2.2.0", platform = "darwin"},
]

# Strategy 3: Extras
[tool.poetry.extras]
gpu = ["torch-gpu-variant"]
cpu = ["torch-cpu-variant"]
```

---

## 5.2 PIP AND VIRTUAL ENVIRONMENTS

### Q: Compare pip, pip-tools, and uv for dependency management.

**Answer:**

```bash
# === pip-tools: Simple and effective ===
# requirements.in (your constraints)
# requirements.txt (locked, generated by pip-compile)

# requirements.in
fastapi>=0.109.0
openai>=1.12.0
pydantic>=2.6.0

# Generate locked requirements
pip-compile requirements.in --output-file requirements.txt
pip-compile requirements-dev.in --output-file requirements-dev.txt

# Install from locked file
pip-sync requirements.txt

# === uv: The fastest Python package manager (2024+, Rust-based) ===
# 10-100x faster than pip

# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create project
uv init ai-project
cd ai-project

# Add dependencies
uv add fastapi openai pydantic
uv add --dev pytest ruff mypy

# Run commands
uv run python script.py
uv run pytest

# Lock and sync
uv lock
uv sync

# Create virtual environment
uv venv
uv venv --python 3.12

# === Virtual environments ===
# Built-in venv
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate     # Windows

# Verify isolation
which python    # Should point to .venv
pip list        # Should show only installed packages

# === conda (for heavy ML dependencies with C/CUDA) ===
conda create -n ai-project python=3.11
conda activate ai-project
conda install pytorch torchvision cudatoolkit=12.1 -c pytorch
pip install openai fastapi  # Use pip for pure Python packages in conda
```

### Q: What is `pyproject.toml` and why is it important?

**Answer:**
`pyproject.toml` (PEP 518, 621, 660) is the unified configuration file for Python projects. It replaces `setup.py`, `setup.cfg`, `MANIFEST.in`, and most tool-specific config files.

```toml
# Single file for ALL project configuration
[project]
name = "ai-service"
version = "1.0.0"
description = "Production AI service"
requires-python = ">=3.11"
dependencies = [
    "fastapi>=0.109.0",
    "openai>=1.12.0",
]

[project.optional-dependencies]
dev = ["pytest>=8.0", "mypy>=1.8"]
ml = ["torch>=2.2", "transformers>=4.38"]

# Tool configurations all in one file
[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"

[tool.mypy]
python_version = "3.11"
strict = true

[tool.ruff]
target-version = "py311"
line-length = 100
```

---

## 5.3 DEPENDENCY MANAGEMENT FOR ML PROJECTS

### Q: What are the challenges of dependency management in ML/AI projects?

**Answer:**

**Key challenges and solutions:**

1. **CUDA/GPU dependencies**: Different machines have different CUDA versions
   ```dockerfile
   # Solution: Use Docker with pinned CUDA base image
   FROM nvidia/cuda:12.1.1-runtime-ubuntu22.04
   # Or use conda for CUDA toolkit management
   ```

2. **Large binary packages**: PyTorch is ~2GB
   ```bash
   # Solution: Use CPU-only variants in CI/dev
   pip install torch --index-url https://download.pytorch.org/whl/cpu
   ```

3. **Reproducibility**: "It worked yesterday"
   ```bash
   # Solution: Lock files + Docker
   poetry lock
   docker build -t ai-service .
   ```

4. **Conflicting transitive dependencies**
   ```bash
   # Solution: Use dependency groups and careful version pinning
   # Check for conflicts: pip check
   ```

**Docker best practices for ML projects:**
```dockerfile
# Multi-stage build for production AI service
FROM python:3.11-slim as builder

# Install build dependencies
RUN pip install poetry==1.8.0
COPY pyproject.toml poetry.lock ./
RUN poetry export -f requirements.txt --without dev --output requirements.txt

FROM python:3.11-slim as runtime

# Copy only the requirements (not poetry itself)
COPY --from=builder requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ /app/src/
WORKDIR /app

# Non-root user for security
RUN useradd --create-home appuser
USER appuser

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## 5.4 LINTING AND FORMATTING

### Q: What is Ruff and why has it replaced flake8, isort, and black for many teams?

**Answer:**
Ruff is a Python linter and formatter written in Rust. It is 10-100x faster than the tools it replaces and supports 800+ lint rules from flake8, isort, pyupgrade, pep8-naming, and more.

```toml
# pyproject.toml -- Ruff configuration
[tool.ruff]
target-version = "py311"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
    "E",    # pycodestyle errors
    "W",    # pycodestyle warnings
    "F",    # pyflakes
    "I",    # isort
    "N",    # pep8-naming
    "UP",   # pyupgrade
    "B",    # flake8-bugbear
    "SIM",  # flake8-simplify
    "TCH",  # flake8-type-checking
    "RUF",  # Ruff-specific rules
    "ANN",  # flake8-annotations
    "ASYNC",# flake8-async
    "S",    # flake8-bandit (security)
    "PTH",  # flake8-use-pathlib
    "ERA",  # eradicate (commented-out code)
]
ignore = [
    "ANN101",  # Missing type annotation for self
    "ANN102",  # Missing type annotation for cls
]

[tool.ruff.lint.isort]
known-first-party = ["ai_service"]

[tool.ruff.lint.per-file-ignores]
"tests/**/*.py" = ["S101", "ANN"]  # Allow assert and skip annotations in tests

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
docstring-code-format = true
```

```bash
# Usage
ruff check .              # Lint
ruff check --fix .        # Auto-fix
ruff format .             # Format (like black)
ruff check --watch .      # Watch mode
```

---

## 5.5 TYPE CHECKING WITH MYPY

### Q: How do you set up mypy for an AI project?

**Answer:**
```toml
# pyproject.toml
[tool.mypy]
python_version = "3.11"
strict = true                          # Enable all strict checks
warn_return_any = true
warn_unused_configs = true
warn_redundant_casts = true
warn_unused_ignores = true
check_untyped_defs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
no_implicit_optional = true
show_error_codes = true
show_column_numbers = true

# Per-module overrides
[[tool.mypy.overrides]]
module = "transformers.*"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "torch.*"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false  # Relax in tests
```

**Common mypy patterns for AI code:**
```python
# Type stubs for untyped libraries
# pip install types-requests types-redis types-PyYAML

# Dealing with numpy/torch types
from numpy.typing import NDArray
import numpy as np

def normalize(vectors: NDArray[np.float32]) -> NDArray[np.float32]:
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / norms  # type: ignore[return-value]  # numpy typing limitation

# TYPE_CHECKING for import-only types (avoids circular imports)
from __future__ import annotations
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from myapp.models import LLMClient

def process(client: LLMClient) -> str:
    ...

# reveal_type() for debugging types
x = some_function()
reveal_type(x)  # mypy will print the inferred type

# cast() when you know better than mypy
from typing import cast
result = cast(list[float], json.loads(response))
```

---

## 5.6 PRE-COMMIT HOOKS

### Q: How do you set up pre-commit hooks for an AI project?

**Answer:**
```yaml
# .pre-commit-config.yaml
repos:
  # Ruff (linting + formatting)
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.0
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format

  # Type checking
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - types-requests
          - pydantic>=2.0

  # Security checks
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.7
    hooks:
      - id: bandit
        args: [-c, pyproject.toml]
        additional_dependencies: ["bandit[toml]"]

  # General hooks
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: check-yaml
      - id: check-json
      - id: check-toml
      - id: end-of-file-fixer
      - id: trailing-whitespace
      - id: check-added-large-files
        args: [--maxkb=1000]  # Prevent accidentally committing model files
      - id: detect-private-key    # Security: catch leaked keys
      - id: check-merge-conflict

  # Detect secrets
  - repo: https://github.com/Yelp/detect-secrets
    rev: v1.4.0
    hooks:
      - id: detect-secrets
        args: [--baseline, .secrets.baseline]

  # Conventional commits
  - repo: https://github.com/compilerla/conventional-pre-commit
    rev: v3.1.0
    hooks:
      - id: conventional-pre-commit
        stages: [commit-msg]
```

```bash
# Setup
pip install pre-commit
pre-commit install              # Install hooks
pre-commit install --hook-type commit-msg  # For commit message hooks
pre-commit run --all-files      # Run on all files (CI)
pre-commit autoupdate           # Update hook versions
```

---

## 5.7 CI/CD PIPELINE FOR AI PROJECTS

```yaml
# .github/workflows/ci.yml
name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pip install ruff
      - run: ruff check .
      - run: ruff format --check .

  type-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pip install poetry && poetry install
      - run: poetry run mypy src/

  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11", "3.12"]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - run: pip install poetry && poetry install --with dev
      - run: poetry run pytest --cov=src --cov-report=xml -v
      - uses: codecov/codecov-action@v4

  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: pip install bandit safety
      - run: bandit -r src/
      - run: safety check
```
