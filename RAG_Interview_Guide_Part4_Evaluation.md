
# RAG Interview Guide
# PART 4: RAG EVALUATION AND THE RAGAS FRAMEWORK

---

## 1. Why RAG Evaluation is Hard

RAG systems have multiple components, each of which can fail independently:
- **Retrieval failure**: Right documents not retrieved
- **Context failure**: Retrieved documents don't contain the answer
- **Generation failure**: LLM doesn't use the context correctly or hallucinates
- **Integration failure**: Components work individually but not together

Traditional NLP metrics (BLEU, ROUGE) are insufficient because:
- RAG answers are generative (many valid phrasings)
- Ground truth answers may not exist
- Quality is multi-dimensional (factuality, completeness, relevance, coherence)

---

## 2. RAG Evaluation Dimensions

### 2.1 Retrieval Evaluation Metrics

**Context Precision (Are retrieved docs relevant?)**
- What fraction of retrieved documents are actually relevant to the question?
- High precision = few irrelevant docs in results
- Formula: `Precision = |Relevant Retrieved| / |Total Retrieved|`

**Context Recall (Are all relevant docs retrieved?)**
- What fraction of all relevant documents were retrieved?
- High recall = few relevant docs missed
- Formula: `Recall = |Relevant Retrieved| / |Total Relevant in Corpus|`

**Mean Reciprocal Rank (MRR)**
- How high is the first relevant document in the results?
- Formula: `MRR = 1/|Q| * sum(1/rank_i)` where rank_i is the rank of the first relevant doc for query i

**Normalized Discounted Cumulative Gain (NDCG)**
- Considers both relevance and position. Higher-ranked relevant docs contribute more.
- Formula: `DCG = sum(rel_i / log2(i + 1))`, normalized by ideal DCG

**Hit Rate / Recall@k**
- Is the relevant document in the top-k results?
- `Hit@k = 1 if relevant doc in top k results, else 0`

```python
# Computing retrieval metrics manually
def compute_retrieval_metrics(queries, ground_truth_docs, retriever, k=5):
    metrics = {"hit_rate": [], "mrr": [], "precision": [], "recall": []}

    for query, relevant_doc_ids in zip(queries, ground_truth_docs):
        retrieved = retriever.invoke(query)
        retrieved_ids = [doc.metadata["id"] for doc in retrieved[:k]]

        # Hit Rate
        hit = any(rid in relevant_doc_ids for rid in retrieved_ids)
        metrics["hit_rate"].append(1.0 if hit else 0.0)

        # MRR
        mrr = 0.0
        for rank, rid in enumerate(retrieved_ids, 1):
            if rid in relevant_doc_ids:
                mrr = 1.0 / rank
                break
        metrics["mrr"].append(mrr)

        # Precision@k
        relevant_retrieved = sum(1 for rid in retrieved_ids if rid in relevant_doc_ids)
        metrics["precision"].append(relevant_retrieved / k)

        # Recall@k
        metrics["recall"].append(relevant_retrieved / len(relevant_doc_ids))

    return {metric: sum(values) / len(values) for metric, values in metrics.items()}
```

### 2.2 Generation Evaluation Metrics

**Faithfulness / Groundedness**
- Is the generated answer factually consistent with the retrieved context?
- Does the answer contain claims NOT supported by the provided context?
- This catches hallucinations.

**Answer Relevancy**
- Does the generated answer actually address the user's question?
- A factually correct answer about the wrong topic scores low.

**Answer Correctness**
- Is the generated answer factually correct compared to ground truth?
- Combines factual accuracy and semantic similarity.

**Answer Completeness**
- Does the answer cover all aspects of the question?
- Partial answers score lower.

---

## 3. The RAGAS Framework

**RAGAS** (Retrieval Augmented Generation Assessment) is the most widely used framework for RAG evaluation, introduced by Shahul Es et al.

**Core Metrics:**

### 3.1 Faithfulness

Measures if the answer is grounded in the given context (no hallucinations).

**How RAGAS computes faithfulness:**
1. Decompose the answer into individual claims/statements
2. For each claim, verify if it can be inferred from the context
3. Faithfulness = (number of supported claims) / (total claims)

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
from datasets import Dataset

# Prepare evaluation dataset
eval_data = {
    "question": [
        "What is the capital of France?",
        "When was Python created?"
    ],
    "answer": [  # Generated by RAG system
        "The capital of France is Paris, which is also the largest city.",
        "Python was created by Guido van Rossum and released in 1991."
    ],
    "contexts": [  # Retrieved contexts
        ["Paris is the capital and most populous city of France."],
        ["Python is a programming language created by Guido van Rossum, first released in 1991."]
    ],
    "ground_truth": [  # Human-annotated ground truth
        "Paris is the capital of France.",
        "Python was created by Guido van Rossum in 1991."
    ]
}

dataset = Dataset.from_dict(eval_data)

# Run evaluation
results = evaluate(
    dataset=dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]
)

print(results)
# {'faithfulness': 0.95, 'answer_relevancy': 0.92, 'context_precision': 0.88, 'context_recall': 0.90}

# Per-sample results
df = results.to_pandas()
print(df)
```

### 3.2 Answer Relevancy

Measures how relevant the answer is to the question. Uses reverse engineering: generates questions from the answer and measures similarity to the original question.

**How it works:**
1. Generate N questions from the answer
2. Compute embedding similarity between each generated question and the original question
3. Answer relevancy = mean similarity

### 3.3 Context Precision

Measures whether the relevant items in the retrieved context are ranked higher. Evaluates the signal-to-noise ratio.

**How it works (with ground truth):**
1. For each retrieved context chunk, determine if it's relevant to the ground truth answer
2. Compute precision at each position
3. Weight by position (higher-ranked relevant chunks score more)

### 3.4 Context Recall

Measures whether all the relevant information needed to answer the question was retrieved.

**How it works:**
1. Decompose ground truth into individual claims
2. For each claim, check if it can be attributed to any retrieved context
3. Context recall = (attributed claims) / (total ground truth claims)

### 3.5 Additional RAGAS Metrics

```python
from ragas.metrics import (
    answer_correctness,
    answer_similarity,
    context_entity_recall,
    noise_sensitivity,
)

# Answer Correctness: Combines factual correctness and semantic similarity
# Uses weighted combination of F1 score on claims and answer similarity

# Context Entity Recall: Are important entities from ground truth present in context?
# Useful for entity-heavy domains

# Noise Sensitivity: How much does the answer change with irrelevant context added?
# Lower is better - robust systems shouldn't be affected by noise

results = evaluate(
    dataset=dataset,
    metrics=[answer_correctness, context_entity_recall]
)
```

---

## 4. LLM-as-Judge Evaluation

Use an LLM to evaluate RAG outputs (commonly GPT-4 or Claude).

```python
from langchain_openai import ChatOpenAI

evaluator_llm = ChatOpenAI(model="gpt-4o", temperature=0)

def evaluate_faithfulness(question, context, answer):
    prompt = f"""You are evaluating the faithfulness of a RAG system's answer.

    Question: {question}
    Context: {context}
    Generated Answer: {answer}

    Evaluate whether the answer is faithful to the context:
    1. Identify each claim in the answer
    2. For each claim, determine if it is supported by the context
    3. Rate faithfulness from 0 to 1

    Output format:
    Claims: [list of claims]
    Supported: [list of yes/no for each claim]
    Score: [0-1]
    Explanation: [brief explanation]"""

    return evaluator_llm.invoke(prompt).content

def evaluate_answer_relevancy(question, answer):
    prompt = f"""Rate how well this answer addresses the question.
    Score from 0 (completely irrelevant) to 1 (perfectly relevant).

    Question: {question}
    Answer: {answer}

    Score:
    Explanation:"""

    return evaluator_llm.invoke(prompt).content

def evaluate_completeness(question, answer, ground_truth):
    prompt = f"""Compare the generated answer against the ground truth answer.
    Rate completeness from 0 to 1.

    Question: {question}
    Ground Truth: {ground_truth}
    Generated Answer: {answer}

    What key points from the ground truth are missing in the generated answer?
    Score:
    Missing points:"""

    return evaluator_llm.invoke(prompt).content

# Pairwise comparison evaluation
def pairwise_comparison(question, answer_a, answer_b):
    prompt = f"""Compare these two answers to the question. Which is better?

    Question: {question}
    Answer A: {answer_a}
    Answer B: {answer_b}

    Evaluate on:
    1. Correctness
    2. Completeness
    3. Clarity
    4. Conciseness

    Winner: [A or B or TIE]
    Explanation:"""

    return evaluator_llm.invoke(prompt).content
```

---

## 5. End-to-End RAG Evaluation Pipeline

```python
import json
from datetime import datetime

class RAGEvaluator:
    def __init__(self, rag_chain, retriever, evaluator_llm):
        self.rag_chain = rag_chain
        self.retriever = retriever
        self.evaluator_llm = evaluator_llm

    def evaluate_dataset(self, eval_dataset):
        """
        eval_dataset: list of {question, ground_truth, [expected_source]}
        """
        results = []

        for item in eval_dataset:
            question = item["question"]
            ground_truth = item["ground_truth"]

            # Get RAG response
            retrieved_docs = self.retriever.invoke(question)
            generated_answer = self.rag_chain.invoke(question)

            # Evaluate retrieval
            retrieval_metrics = self.evaluate_retrieval(
                question, retrieved_docs, ground_truth,
                item.get("expected_source")
            )

            # Evaluate generation
            generation_metrics = self.evaluate_generation(
                question, generated_answer, retrieved_docs, ground_truth
            )

            results.append({
                "question": question,
                "ground_truth": ground_truth,
                "generated_answer": generated_answer,
                "num_docs_retrieved": len(retrieved_docs),
                **retrieval_metrics,
                **generation_metrics
            })

        # Aggregate metrics
        aggregate = self.compute_aggregate(results)
        return results, aggregate

    def evaluate_retrieval(self, question, docs, ground_truth, expected_source=None):
        # Check if context contains the answer
        context = "\n".join(doc.page_content for doc in docs)

        context_relevance_prompt = f"""Does the following context contain enough information
        to answer the question? Rate from 0 to 1.
        Question: {question}
        Ground Truth Answer: {ground_truth}
        Context: {context}
        Score:"""

        score = self.evaluator_llm.invoke(context_relevance_prompt).content

        metrics = {"context_sufficiency": float(score.strip().split("\n")[0])}

        # Source accuracy (if expected source provided)
        if expected_source:
            sources = [doc.metadata.get("source", "") for doc in docs]
            metrics["source_hit"] = 1.0 if expected_source in sources else 0.0

        return metrics

    def evaluate_generation(self, question, answer, docs, ground_truth):
        context = "\n".join(doc.page_content for doc in docs)

        # Faithfulness
        faith_prompt = f"""Rate the faithfulness of the answer to the context (0-1).
        Is every claim in the answer supported by the context?
        Context: {context}
        Answer: {answer}
        Score:"""

        # Relevancy
        rel_prompt = f"""Rate how well the answer addresses the question (0-1).
        Question: {question}
        Answer: {answer}
        Score:"""

        # Correctness
        correct_prompt = f"""Rate the factual correctness compared to ground truth (0-1).
        Ground Truth: {ground_truth}
        Generated: {answer}
        Score:"""

        faithfulness = float(self.evaluator_llm.invoke(faith_prompt).content.strip().split("\n")[0])
        relevancy = float(self.evaluator_llm.invoke(rel_prompt).content.strip().split("\n")[0])
        correctness = float(self.evaluator_llm.invoke(correct_prompt).content.strip().split("\n")[0])

        return {
            "faithfulness": faithfulness,
            "answer_relevancy": relevancy,
            "answer_correctness": correctness
        }

    def compute_aggregate(self, results):
        metrics = ["faithfulness", "answer_relevancy", "answer_correctness", "context_sufficiency"]
        aggregate = {}
        for metric in metrics:
            values = [r[metric] for r in results if metric in r]
            if values:
                aggregate[f"avg_{metric}"] = sum(values) / len(values)
                aggregate[f"min_{metric}"] = min(values)
        return aggregate
```

---

## 6. Building Evaluation Datasets

### Synthetic Test Generation:

```python
from ragas.testset.generator import TestsetGenerator
from ragas.testset.evolutions import simple, reasoning, multi_context

# Generate synthetic test set from your documents
generator = TestsetGenerator.from_langchain(
    generator_llm=ChatOpenAI(model="gpt-4o"),
    critic_llm=ChatOpenAI(model="gpt-4o"),
    embeddings=OpenAIEmbeddings()
)

testset = generator.generate_with_langchain_docs(
    documents=documents,
    test_size=50,
    distributions={
        simple: 0.4,          # Simple factual questions
        reasoning: 0.3,       # Questions requiring reasoning
        multi_context: 0.3    # Questions requiring info from multiple docs
    }
)

test_df = testset.to_pandas()
```

### Manual Evaluation Dataset:

```python
eval_dataset = [
    {
        "question": "What is the company's return policy?",
        "ground_truth": "The company offers a 30-day return policy for all unused items with original packaging.",
        "expected_source": "return_policy.pdf",
        "difficulty": "simple",
        "category": "policy"
    },
    {
        "question": "How does the annual bonus calculation differ between engineering and sales teams?",
        "ground_truth": "Engineering bonuses are based on 70% performance review and 30% company performance. Sales bonuses are based on 80% quota attainment and 20% company performance.",
        "expected_source": "compensation_guide.pdf",
        "difficulty": "multi_context",  # Requires info from multiple sections
        "category": "compensation"
    },
    # ... more examples covering:
    # - Different question types (factual, comparative, multi-hop)
    # - Different difficulty levels
    # - Edge cases (no answer available, conflicting info)
    # - Different document sources
]
```

---

## 7. Continuous Evaluation and Monitoring

```python
# Production monitoring setup
class RAGMonitor:
    def __init__(self):
        self.metrics_log = []

    def log_query(self, query, retrieved_docs, response, latency_ms, user_feedback=None):
        entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "num_docs_retrieved": len(retrieved_docs),
            "top_doc_score": retrieved_docs[0].metadata.get("score", None) if retrieved_docs else None,
            "response_length": len(response),
            "latency_ms": latency_ms,
            "user_feedback": user_feedback,  # thumbs up/down
        }
        self.metrics_log.append(entry)

        # Alert if quality degrades
        if self.detect_degradation():
            self.send_alert("RAG quality degradation detected")

    def detect_degradation(self, window=100):
        """Check if recent queries show quality drop."""
        if len(self.metrics_log) < window:
            return False

        recent = self.metrics_log[-window:]
        feedback_scores = [e["user_feedback"] for e in recent if e["user_feedback"] is not None]

        if feedback_scores:
            recent_satisfaction = sum(1 for f in feedback_scores if f == "positive") / len(feedback_scores)
            if recent_satisfaction < 0.7:  # Below 70% satisfaction
                return True
        return False
```

---

## 8. Evaluation Interview Questions

### Q: "How do you evaluate a RAG system when you don't have ground truth labels?"

**Answer:**
1. **LLM-as-Judge**: Use GPT-4 to evaluate faithfulness and relevancy without ground truth
2. **Faithfulness evaluation**: Only requires context + answer (no ground truth needed). Check if answer is supported by context.
3. **User feedback**: Implement thumbs up/down, collect implicit signals (follow-up questions indicate dissatisfaction)
4. **Synthetic evaluation sets**: Generate QA pairs from your documents using LLMs
5. **A/B testing**: Compare system variations on real traffic
6. **Retrieval sanity checks**: Manually review top-k results for a sample of queries
7. **Hallucination detection**: Cross-reference claims in answers against retrieved contexts

### Q: "What is the difference between faithfulness and answer correctness?"

**Answer:**
- **Faithfulness**: Is the answer consistent with the PROVIDED CONTEXT? An answer can be faithful (everything it says comes from the context) but incorrect (the context itself contains wrong information).
- **Answer Correctness**: Is the answer factually correct compared to GROUND TRUTH? This is an absolute measure of accuracy.
- Example: If context says "Paris is in Germany" and answer says "Paris is in Germany" - faithfulness is HIGH (answer follows context) but correctness is LOW (factually wrong).

### Q: "How do you handle evaluation at scale in production?"

**Answer:**
1. **Sampling**: Evaluate a random sample (5-10%) of production queries in detail
2. **Automated metrics**: Run faithfulness checks on all responses asynchronously
3. **User feedback integration**: Track thumbs up/down with query-level granularity
4. **Regression testing**: Maintain a curated test set, run before every deployment
5. **Drift detection**: Monitor retrieval score distributions over time
6. **A/B testing framework**: Route percentage of traffic to new model/config
7. **Escalation pipeline**: Low-confidence responses flagged for human review
